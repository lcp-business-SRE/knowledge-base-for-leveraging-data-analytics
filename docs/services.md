# データ分析を実現するための技術

## この資料の役割

データ分析を実現するためには、エコシステムの恩恵を受ける必要がある。

なぜならば、データ分析界隈の技術は複雑かつ技術スピードは非常に早いため、独自に開発することは、すぐに技術的負債を受けてしまう可能性が高いからである。

そのため、今はどの会社も様々なサービスを組わせてデータ分析を行ない、時代やビジネスの変化に合わせて柔軟に変化してい仕組を整えるのが主流である。

この資料では、**いまよく使われているサービスを整理し、技術選定に役立つための情報を一元管理する**ことで選定時に戦略をもって判断できるようにすることを目的にする。

## 目次

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=3 orderedList=false} -->

<!-- code_chunk_output -->

- [データ分析を実現するための技術](#データ分析を実現するための技術)
  - [この資料の役割](#この資料の役割)
  - [目次](#目次)
  - [技術比較表（サマリ）](#技術比較表サマリ)
  - [整理のしかた](#整理のしかた)
  - [データ収集](#データ収集)
  - [データ保存](#データ保存)
    - [Amazon S3](#amazon-s3)
  - [データ加工](#データ加工)
    - [dbt（Data Build Tool）](#dbtdata-build-tool)
  - [データ分析・活用](#データ分析活用)
    - [Google Looker Studio](#google-looker-studio)
    - [Apache Superset](#apache-superset)
    - [雛形](#雛形)

<!-- /code_chunk_output -->

## 技術比較表（サマリ）

TODO:以下で整理した内容をマトリクスで比較表としてまとめる、マトリクス要素については要相談。

## 整理のしかた

データ分析を行うにあたり必要なデータ分析基盤は４つの要素で分類されるため、以下４分類ごとに整理することにする。

- データ収集
- データ保存
- データ加工
- データ分析・活用

※ただし、サービスによっては複数を跨いで対応できる場合があるため、その場合はメイン機能で整理する

---

## データ収集

データ分析のためには、業務システムや顧客データなどから必要なデータを集めます。収集したデータはそのままでは使えないことが多いため、適切に保存し、用途に応じて加工します。

---

## データ保存

データ分析に必要なデータは、適切に保存する必要があります。これを「データレイク」と呼びます。データ元によっては一時的なログしか発生しない場合もあり、取得を逃すと再取得が困難です。そのため、必要なデータを収集し、保存しておくことが重要です。

### Amazon S3

#### 1. できることの概要

- データの保存
- 外部からのアクセス
- 簡易的なクエリ
  - Glue等のほかのサービスと組み合わせるとSQLクエリも可能
  - S3 Tablesという新機能を使うと、Athena経由で直接SQLクエリができるようになる。
- オブジェクトの保存などの特定イベントをトリガーとし、自動的な処理が可能
- [メタデータのマネージドテーブル化](https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/UsingMetadata.html)
  - これにより、データの捜索やガバナンスが容易になる

#### 2. サービスの特徴

- AWSの基本ストレージのため、資料が豊富
- フルマネージド
- スケーラビリティが極めて高い
  - 実用上は無制限に増やせる。
- 保存データの利用法に応じたプランが存在する
  - S3 標準
  - S3 標準 - 低頻度アクセス
  - S3 Glacier Instant Retrieval
  - S3 Glacier Deep Archive (標準/大容量)
  - S3 One Zone - 低頻度アクセス
  - Amazon S3 Glacier Flexible Retrieval
  - S3 Intelligent-Tiering
- ウェブサイトのホスティングも可能

#### 3. 価格

|  金額はすべて米ドル   |  標準   | 標準 - 低頻度アクセス  |  S3 Glacier Instant Retrieval   |  S3 Glacier Deep Archive    | S3 One Zone - 低頻度アクセス | Amazon S3 Glacier Flexible Retrieval |S3 Intelligent-Tiering|
| --- | --- | --- | --- | --- | --- | --- | --- |
|  概要   |  標準的なデータ向け   |   アクセス頻度が低いデータ向け  |   アクセス頻度がさらに低いデータ向け  |  年数回未満レベルのアクセスで、急を要しないデータ向け   |再作成可能なデータ向け | 年数回レベルのアクセスするデータで、緊急の取り出しが必要かもしれないもの  | アクセス頻度等でストレージを振り分ける |
|  データ容量に対する価格(/GB)   |  0.025   |  0.0138   |   0.005  |   0.002  | 0.011 |0.0045  |0.025～0.005(高頻度～アーカイブ)|
|  データ取り出しに対する価格(/1000リクエスト)   |  0   |   0  |  0   |  0.1142/0.025  (標準/大容量) | 0| 11.00/0.0571/0 (迅速/標準/大容量)|原則 0 </br> アーカイブアクセスかつ迅速　11.00|
|  データ取り出しに対する価格(/GB)   |  0   |   0.01  |  0.03   |  0.022/0.005  (標準/大容量) | 0.01 | 0.033/0.011/0 (迅速/標準/大容量)|原則 0 </br> アーカイブアクセスかつ迅速　0.033|
|  PUT、COPY、POST、LIST リクエスト(/1000リクエスト)   |   0.0047  |   0.01  |  0.02   |   0.065  | 0.01| 0.03426 |0.0047|
|  GET、SELECT、他のすべてのリクエスト (/1000リクエスト)   |  0.00037 |   0.001  |  0.01   |  0.00037 | 0.001| 0.00037 |0.00037|
|  外部へのデータ送信(/GB)   |   0.114  |  0.114   |   0.114  |  0.114   | 0.114 |  0.114  |0.114|
|  データ取り出しにかかる時間スケール   |   ms  |   ms  |   ms  |    12時間以内 | ms | 1分～12時間(迅速～大容量) |振り分けられたストレージによる|
|ライフサイクルによる移行の受け入れ|-|0.01|0.02|0.065|0.01|0.03426|0.01|
|  備考   |     |     |     |  決済データなどの法的に長期保存を要求されるデータ向け   | AZ1つにしか保存されないため、安価。復帰可能なものや消えてもいいもの向け| 基本はアクセス頻度が低く急がないが、例外的に急いで取り出すこともあるデータ向け。</br>素早く取り出そうとするほど料金が跳ね上がる。 |モニタリングおよびオートメーションにも課金。1000件当たり0.0025 USD</br>128KB未満のデータは振り分け対象外</br>頻度最低のストレージに向けて優先リクエストを行うと課金。|

#### 4. 技術的に優れていること

- 高耐久性
  - One Zoneは99.99%
  - それ以外は99.999999999%
- ライフサイクルが設定可能。これにより、決められた期間を経過したファイルを削除もしくは安価なストレージに移行することができる。
- バージョン管理が可能
  - 使用容量はその分増える。
- アクセス管理機能を用いてアクセスを制限できる。
- ファイルに対する一時リンクを作成可能
- S3 Intelligent-Tieringの場合、自動でストレージクラスを振り分けてくれる。この機能によりコスト削減が見込める。

#### 5. 制約事項

- 本質的にはディレクトリ構造を持たない
  - 全ファイルはファイルパス＝名前として管理されているので、フォルダ名変更が難しい。
- バケット名は同一パーティション内(cnとus-gov以外全てを指す)でユニークである必要がある。
- Glacierは大量の軽量オブジェクトを保存したり、短期間で削除したりすると通常のストレージよりも高額になる罠がある。
  - 軽量オブジェクト保存時の問題
    - ファイルごとに40KB程度のメタデータが必ずつくので、小さなファイルを大量に保存するとファイルサイズの増加による料金増加が単価低下を上回る。[1](https://www.hands-lab.com/tech/t13536/)
    - ライフサイクルポリシー等でstandardから移行させる場合、GlacierへのPUTリクエストの料金が高額になる場合がある。ファイルサイズと保存期間によっては転送料金で単価低下分を超える費用がかかる。[2](https://qiita.com/Ichi0124/items/19a05ea599bd13372586)
  - 短期間使用
    - 最低保存期限があり、それに満たない期間で削除した場合90日(Deep Archiveなら180日)分の保存代金との差額が一気に請求される(1日目に消すと、3か月分の請求が一月にまとまってされる)。[3](https://qiita.com/daktu32/items/2dbab869dca6a3603e5d)
- SQLによるクエリをする際に制限がある。
  - 他のサービスを連携する必要がある。
  - 速度が出ない。
    - S3 Tablesを使うと高速化可能。
    - 特定の形式のS3 Tablesの場合、[Iceberg REST API](https://github.com/apache/iceberg/blob/main/open-api/rest-catalog-open-api.yaml)形式で直接クエリも可能。しかし、複数テーブルを触るユースケースではAWS Glue Iceberg REST エンドポイントを使用するべきとされている。

#### 6. 他社事例

- [データレイクを構築](https://aws.amazon.com/solutions/case-studies/salesforce-amazons3-intelligent-tiering-case-study/?did=cr_card&trk=cr_card)
- [データをアーカイブ](https://aws.amazon.com/jp/solutions/case-studies/bbc-s3-case-study/)

#### 7. 世の中の評価・評判

- 無制限の容量と、データ耐久性の高さが評価されてる
- 安い
- クラウドストレージのデファクトスタンダードとなっている。
  - APIがS3 APIと互換を持つサービスも存在する。
- みんな使っているので、学習コストは低い。

#### 8. 用途についての所感

- AWSでストレージを使うならよほどのことがあってもS3を使わない構成は考えられない
- 料金を最適化は程々までにしたほうが良い。
  - ファイルサイズ・アクセス頻度・保存期間などによりベストプラクティスも変わる。コスト削減のためのアプローチ方法は多いが、効果的な方法を見つけるのは難しい。とりあえずは、S3 Intelligent-Tieringの自動分類に任せるのが無難か。
    - コスト削減ができそうな部分
      - 定期的な運用の見直し
      - ファイル転送回数削減
      - ファイルの適切な圧縮
- S3 Tablesは可能性を感じるが、普通のSQLはAthena経由でないと動かないのが悲しい

#### 9. 備考

- バケット名は一意になる性質による[事故](https://gigazine.net/news/20240502-how-empty-s3-bucket-aws-bill-explode/)や、その性質を悪用した[悪意あるバケット](https://qiita.com/mj69/items/87094a7fb5f492a0a97a)もあるので注意。
<!-- markdownlint-disable MD033 -->
- メタデータのマネージドテーブル化は **<span style="color:rgb(225, 0, 0)">2025/05/26現在日本リージョンでは未提供</span>**。
<!-- markdownlint-disable MD033 -->

#### 10. 参考サイト

- [Amazon S3](https://aws.amazon.com/jp/s3/)
- [AWS SDK JavaScript v3でS3のファイル操作 チートシート](https://tmokmss.hatenablog.com/entry/20230118/1674010626)

---

## データ加工

データレイクに保存された生データはそのままでは分析に適していないため、適切な形式に加工する必要があります。加工されたデータを保存する場所を「データウェアハウス」と呼びます。さらに、用途に応じて加工・区別されたデータを保存する場所は「データマート」と呼ばれます。これらのデータ加工の工程を「ETL」（Extract, Transform, Load）と呼びます。

### dbt（Data Build Tool）

キーワード：ELTツール

#### 1. できることの概要

dbt Labs社が開発しているデータ連携のELT（データストア内でのデータ変換）の実施をサポートで
OSS版のdbtでも機能が充実しており、データエンジニアの方には一部の機能の利用だけでも価値があるツール

- データウェアハウス内のデータ変換処理をSQLで記述、実行
- データモデルの開発、テスト、ドキュメント化
- データ変換の依存関係管理、自動化
- データ品質の監視、データリネージの可視化

#### 2. サービスの特徴

- SQL中心のアプローチ：SQLに慣れたデータアナリストでも容易に利用可能
- モジュール性、再利用性：データ変換処理をモジュール化し、再利用可能
- テスト機能：データ品質を担保するためのテスト機能を搭載
- ドキュメント自動生成：データモデルのドキュメントを自動生成
- バージョン管理：Gitと連携し、データ変換処理のバージョン管理が可能
- dbt Core(オープンソース)とdbt Cloud(有償SaaS)がある。

![特徴](./assets/dbt-info.png)
[引用元 : データエンジニア界隈で話題のdbt（data build tool）のまとめ](https://qiita.com/manabian/items/67af7e4476d436aded77)

細かい特徴

- データの実体化方法としては、ビューの生成、CTAS（CREATE TABLE AS SLECT）、増分更新、差分更新（MERGE）、差分履歴更新（SCD Type2相当）がある。カスタマイズも可能であり、データ連携先データストアの製品特徴に応じて処理を自動で実施
- SQL文を発行するツールであることから、dbtの実行環境のスケールをあまり考慮する必要はない。データをロードする機能は基本的にはなく、データ連携済みであることが前提。
- 結合キーは単一列であることが推奨されており、複数カラムの値をハッシュ化する方法（[Generating Surrogate Keys Across Warehouses | dbt Developer Blog (getdbt.com)](https://docs.getdbt.com/blog/sql-surrogate-keys)）が紹介されている。
- テスト機能にて、ユニークキー制約性チェック、NOT NULL制約チェック、ドメイン制約、及び、外部キー制約チェックがあり、SQLによりテストケースを作成することが可能。dbtにはデータロード機能がないことから、システムテストの実施は別途必要
- dbt_utils（[dbt - Package hub (getdbt.com)](https://hub.getdbt.com/dbt-labs/dbt_utils/0.1.7/)）などの公開されているパッケージを利用することで、先駆者の知見を拝借可能。
- ドキュメント機能にて、データ連携に関する情報だけでなく、データ活用に関する情報（[Exposures](https://docs.getdbt.com/docs/building-a-dbt-project/exposures) 、[Analyses](https://docs.getdbt.com/docs/building-a-dbt-project/analyses)、[Metrics](https://docs.getdbt.com/docs/building-a-dbt-project/metrics)等）を、yaml、あるいは、Markdownで記述可能。

#### 3. 価格

- dbt Core：オープンソースであり、無償で利用可能
- dbt Cloud:有料のSaaSモデルで、機能とチームのシート数によって価格が変わります。

#### 4. 技術的に優れていること

- SQLに特化：データアナリストのスキルを最大限に活用
- 依存関係管理：データ変換処理の依存関係を自動で解決
- テスト機能：データ品質を担保するためのテスト機能を標準搭載
- ドキュメント自動生成：データモデルのドキュメントを常に最新の状態に保つ
- モダンデータスタックとの親和性：Snowflake、BigQuery、Redshiftなどの主要なクラウドデータウェアハウスとの連携が容易。

#### 5. 制約事項

- SQLの知識が必要：SQLの知識がないと利用が難しい
- 複雑なデータ変換には不向き：複雑なデータ変換には、Pythonなどのプログラミング言語が必要になる場合がある。
- dbt CloudはSaaSであるため、クラウド環境に依存する。

#### 6. 他社事例

- 多くのデータドリブン企業が、データ分析基盤にdbtを採用
  - 例：Netflix,Gitlab,doda,SmartHR

#### 7. 世の中の評価・評判

- データ分析基盤のデファクトスタンダードとして、高い評価を得ている
- データアナリスト、データエンジニアの生産性を向上させるツールとして人気

#### 8. 用途についての所感

- データウェアハウス内のデータ変換、データマート構築に最適
- データ分析基盤の構築、運用効率化に貢献
- データ品質向上、データリネージ可視化など、データガバナンス強化にも有効

#### 9. 備考

- dbt Coreはコマンドラインツール、dbt CloudはWebベースのプラットフォーム
- dbt Learnという教育コンテンツも充実している。

#### 10. 参考サイト

- [dbt Labs](https://www.getdbt.com/)
- [dbt Japan User Group](https://www.getdbt.com/community/local-communities/japan/)
- [dbtとは？｜dbtで始めるデータパイプライン構築〜入門から実践〜 - Zen](https://zenn.dev/dbt_tokyo/books/537de43829f3a0/viewer/what_dbt)
- [データエンジニア界隈で話題のdbt（data build tool）のまとめ](https://qiita.com/manabian/items/67af7e4476d436aded77)
- [ドキュメントの最初のページ。](https://docs.getdbt.com/docs/introduction)
- [dbtがどういったものであるかが説明](https://blog.getdbt.com/what-exactly-is-dbt/)
- [基本チュートリアル。](https://docs.getdbt.com/tutorial/setting-up)
- [dbtに関するよくある質問に対する回答集。](https://docs.getdbt.com/faqs/all)
- [無償のオンライントレーニング。](https://courses.getdbt.com/collections)
- [利用可能なデータストアサービスの記事。](https://docs.getdbt.com/docs/available-adapters)
- [データ活用におけるデータエンジニアリングを実施する際のガイド](https://www.getdbt.com/analytics-engineering/)

---

## データ分析・活用

データウェアハウスに保管されたデータは、そのままでは意味を持ちません。戦略立案や経営判断に役立てるためには、グラフや表にまとめる必要があります。これを行うツールはBIツール（Business Intelligence）と呼ばれます。また、整理されたデータは機械学習にも活用できます。

### Google Looker Studio

![Lokerstudio](./assets/looker-studio-logo.png)
[Looker Studio へようこそ](https://cloud.google.com/looker/docs/studio?hl=ja)

キーワード：BI

#### 1. できることの概要

- データ可視化
  - Googleアナリティクス、Google広告、Googleスプレッドシートなど、多様なデータソースからデータを抽出し、グラフや表などの視覚的なレポートを作成できる
  - ドラッグ＆ドロップの簡単な操作で、誰でも美しいダッシュボードを作成できる
- データ共有
  - 作成したレポートは、URLやPDFで簡単に共有できる
  - チームメンバーと共同編集も可能
- リアルタイム連携
  - データソースとリアルタイムで連携し、常に最新の情報を表示できる

#### 2. サービスの特徴

![サンプル](./assets/LookerStudio-sample.png)

#### 3. 価格

- 基本無料
  - より大規模なデータ分析を行いたい場合にはGoogleの有料BIツールであるLookerを利用

#### 4. 技術的に優れていること

- Googleの強力なインフラ
  - Googleのクラウド基盤を利用しており、高速で安定した動作
- 豊富なコネクタ
  - 多様なデータソースとの接続を可能にするコネクタが用意
- カスタマイズ性
  - 豊富なグラフや表のオプションがあり、細かいカスタマイズが可能

#### 5. 制約事項

- 複雑なデータ分析には不向き
  - Lookerのような高度なデータモデリングや分析機能は備えていない
- 大規模データ処理
  - 非常に大規模なデータセットを扱う場合、パフォーマンスに影響が出る可能性がある

#### 6. 他社事例

- デジタルマーケティングレポート
- ウェブサイトのパフォーマンス分析
- 売上ダッシュボード
- 広告レポート

#### 7. 世の中の評価・評判

- 「無料なのに高機能で使いやすい」
- 「データの可視化が簡単にできるので、重宝している」
- 「チームでの情報共有がスムーズになった」
- 「データに基づいた意思決定がしやすくなった」

#### 8. 用途についての所感

- Google アカウントが必要なので社内利用には向いている
- データ分析の専門家でなくても、簡単にデータを可視化し、共有できるのは強み
- 機密情報の扱いは調査が必要
- 案件で利用することはセキュリティーと仕組み上厳しい（とりあえず見せるなどは可能かと）

#### 9. 備考

- 以前は「Googleデータポータル」という名称でしたが、2022年10月に「Google Looker Studio」に名称変更された

#### 10. 参考サイト

- [Looker Studio へようこそ | Google Cloud](https://cloud.google.com/looker/docs/studio?hl=ja)
- [GoogleのLooker Studioとは？機能やメリット、使いこなすコツを解説！ - SEOタイムズ:](https://seotimes.jp/looker-studio/)
- [【初心者向け】図解あり！Google Looker Studioの使い方を解説 - PLAN-B:](https://www.plan-b.co.jp/blog/seo/31762/)

### Apache Superset

### 雛形
<!-- markdownlint-disable MD022 -->
#### 1. できることの概要
#### 2. サービスの特徴
#### 3. 価格
#### 4. 技術的に優れていること
#### 5. 制約事項
#### 6. 他社事例
#### 7. 世の中の評価・評判
#### 8. 用途についての所感
#### 9. 備考
#### 10. 参考サイト
<!-- markdownlint-enable MD022 -->
